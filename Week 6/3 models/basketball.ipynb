{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\this pc\\anaconda3\\lib\\site-packages (1.22.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.23.3-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\This PC\\\\anaconda3\\\\Lib\\\\site-packages\\\\~-mpy\\\\.libs\\\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: river in c:\\users\\this pc\\appdata\\roaming\\python\\python39\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy>=1.22 in c:\\users\\this pc\\anaconda3\\lib\\site-packages (from river) (1.23.3)\n",
      "Requirement already satisfied: pandas>=1.3 in c:\\users\\this pc\\anaconda3\\lib\\site-packages (from river) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.5 in c:\\users\\this pc\\anaconda3\\lib\\site-packages (from river) (1.7.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\this pc\\anaconda3\\lib\\site-packages (from pandas>=1.3->river) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\this pc\\anaconda3\\lib\\site-packages (from pandas>=1.3->river) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\this pc\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.3->river) (1.16.0)\n",
      "Collecting numpy>=1.22\n",
      "  Using cached numpy-1.22.4-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.3\n",
      "    Uninstalling numpy-1.23.3:\n",
      "      Successfully uninstalled numpy-1.23.3\n",
      "Successfully installed numpy-1.22.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.22.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U numpy\n",
    "!pip install river\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m__init__.pxd:942\u001b[0m, in \u001b[0;36mnumpy.import_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mriver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compose\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mriver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_model\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mriver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mriver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mriver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\river\\metrics\\__init__.py:48\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmcc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MCC\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MSE, RMSE, RMSLE\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmutual_info\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdjustedMutualInfo, MutualInfo, NormalizedMutualInfo\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprecision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MacroPrecision, MicroPrecision, Precision, WeightedPrecision\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mr2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m R2\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\river\\metrics\\mutual_info.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mriver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpected_mutual_info\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m expected_mutual_info\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdjustedMutualInfo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMutualInfo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormalizedMutualInfo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m ]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMutualInfo\u001b[39;00m(metrics\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mMultiClassMetric):\n",
      "File \u001b[1;32mbuild\\lib.win-amd64-cpython-38\\river\\metrics\\expected_mutual_info.pyx:13\u001b[0m, in \u001b[0;36minit river.metrics.expected_mutual_info\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m__init__.pxd:944\u001b[0m, in \u001b[0;36mnumpy.import_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\"basketball_dreb.csv\")\n",
    "\n",
    "dataset.head()\n",
    "print(type(dataset))\n",
    "\n",
    "params = {'converters': {'value': float},'parse_dates': {'time': \"%Y-%m-%d %H:%M:%S\"}}\n",
    "\n",
    "dataset = dict()\n",
    "\n",
    "from river import stream\n",
    "from river import compose\n",
    "from river import linear_model\n",
    "from river import metrics\n",
    "from river import evaluate\n",
    "from river import preprocessing\n",
    "from river import feature_extraction\n",
    "from river import stats\n",
    "from river import optim\n",
    "from river import facto\n",
    "from river import model_selection\n",
    "from river import time_series\n",
    "from river import tree\n",
    "from river import dummy\n",
    "\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "hour_list = [str(i) for i in range (0,24)]\n",
    "minute_list = [f'{str(i)} min' for i in range (0, 60)]\n",
    "def get_hour(x):\n",
    "    x['h'] = x['time'].hour\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_minute(x):\n",
    "    x['m'] = x['time'].minute//15 + 1\n",
    "    return x\n",
    "\n",
    "def get_day(x):\n",
    "    \n",
    "    return {'d' : x['time'].day}\n",
    "\n",
    "def get_hour_sin_and_cos(x):\n",
    "    x['sin_h'] = np.sin(np.pi*(x['time'].hour)/12) \n",
    "    x['cos_h'] = np.cos(np.pi*(x['time'].hour)/12)\n",
    "    return {'sin_h' : np.sin(np.pi*(x['time'].hour)/12), 'cos_h': np.cos(np.pi*(x['time'].hour)/12)}\n",
    "\n",
    "\n",
    "def get_minute_distances(x):\n",
    "    x['sin_m'] = np.sin(np.pi*(x['time'].minute)/30)\n",
    "    x['cos_m'] = np.cos(np.pi*(x['time'].minute)/30)\n",
    "    return {'sin_m' : np.sin(np.pi*(x['time'].minute)/30), 'cos_m': np.cos(np.pi*(x['time'].minute)/30)}\n",
    "\n",
    "def get_date_progress(x):\n",
    "    return {'date': x['time'].toordinal() - datetime.datetime(2022, 1, 1, 0, 0).toordinal()}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "temp = temp = [30 for _ in range (30)]\n",
    "cache = [temp]\n",
    "my_dict = {}\n",
    "for x, y in stream.iter_csv('basketball_dreb.csv', target = 'value', **params):\n",
    "    if i < 30:\n",
    "        t = temp.copy()\n",
    "        t[i] = y\n",
    "        cache.append(t)      \n",
    "        temp = t\n",
    "        my_dict[x['time']] = t\n",
    "    else:\n",
    "        t = temp.copy()\n",
    "        t.pop(0)\n",
    "        t.append(y)\n",
    "        cache.append(t)\n",
    "        temp = t\n",
    "        my_dict[x['time']] = t\n",
    "    i += 1\n",
    "\n",
    "\n",
    "def get_lag(x):\n",
    "    lag_values = my_dict[x['time']]\n",
    "    return {f'lag_{i}': lag_values[i] for i in range (30)}\n",
    "    \n",
    "\n",
    "\n",
    "models = [linear_model.LinearRegression(optimizer=optim.SGD(lr=lr)) for lr in [0.05, 0.02, 0.01, 0.005, 0.002, 0.0001]]\n",
    "\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('date_progress', compose.FuncTransformer(get_date_progress)),\n",
    "        ('lags', compose.FuncTransformer(get_lag))\n",
    "    )))\n",
    "\n",
    "model += (\n",
    "    get_hour | \n",
    "        feature_extraction.TargetAgg(\n",
    "            by=['h'], how=stats.Mean()\n",
    "\n",
    "\n",
    "))\n",
    "model += (\n",
    "    get_minute | \n",
    "        feature_extraction.TargetAgg(\n",
    "            by=['m'], how=stats.Mean()\n",
    "\n",
    "\n",
    "))\n",
    "\n",
    "model |=  preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression(intercept_lr=0.3)\n",
    "\n",
    "\n",
    "# model |=  model_selection.EpsilonGreedyRegressor(models, epsilon=0.025, decay=0.1, burn_in=100, seed=1)\n",
    "# model |= tree.HoeffdingAdaptiveTreeRegressor(grace_period=100, leaf_prediction='adaptive', model_selector_decay=0.9, seed=0)\n",
    "# model = preprocessing.TargetStandardScaler(regressor=model)\n",
    "\n",
    "\n",
    "\n",
    "metric = metrics.MAE() + metrics.R2()\n",
    "evaluate.progressive_val_score(stream.iter_csv('basketball_dreb.csv', target = 'value', **params), model, metric, print_every=50)\n",
    "# evaluate.progressive_val_score(stream.iter_csv('hospital_wait.csv', target = 'value', **params), dummy.StatisticRegressor(stats.Shift(1)), metric, print_every=50)\n",
    "model.transform_one(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "\n",
    "queue = collections.deque([], 4)\n",
    "\n",
    "def evaluate_model(model): \n",
    "\n",
    "    metric = metrics.Rolling(metrics.MAE(), 10)\n",
    "    metric_b = metrics.Rolling(metrics.MAE(), 10)\n",
    "    \n",
    "    dates = []\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    \n",
    "    baseline = 0\n",
    "    y_baseline = []\n",
    "    for x, y in stream.iter_csv('basketball_dreb.csv', target = 'value', **params):\n",
    "        \n",
    "        new_feats = {f\"lag_{i}\": v for i, v in enumerate(queue)}\n",
    "\n",
    "        # copy of x\n",
    "        x_ = dict(x)\n",
    "        x_.update(new_feats)\n",
    "\n",
    "        y_pred = model.predict_one(x_)\n",
    "        model.learn_one(x_, y)\n",
    "\n",
    "        queue.append(y)\n",
    "\n",
    "        # Obtain the prior prediction and update the model in one go\n",
    "        y_pred = model.predict_one(x)\n",
    "        model.learn_one(x, y)\n",
    "\n",
    "        # Update the error metric\n",
    "        metric.update(y, y_pred)\n",
    "        metric_b.update(y, baseline)\n",
    "        \n",
    "        # Store the true value and the prediction\n",
    "        dates.append(x['time'])\n",
    "        y_trues.append(y)\n",
    "        y_preds.append(y_pred)\n",
    "        y_baseline.append(baseline)\n",
    "        baseline = y\n",
    "        \n",
    "    print(metric, metric_b)\n",
    "\n",
    "    # Plot the results\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    ax.grid(alpha=0.75)\n",
    "    ax.plot(dates, y_trues, lw=3, color='#2ecc71', alpha=800, label='Ground truth')\n",
    "#     ax.plot(dates, y_preds, lw=3, color='#e74c3c', alpha=800, label='Prediction')\n",
    "    ax.plot(dates, y_baseline, lw=3, color='#e74c3c', alpha=800, label='Baseline')\n",
    "    ax.legend()\n",
    "    ax.set_title(metric)\n",
    "evaluate_model(model)\n",
    "\n",
    "\n",
    "def make_model(alpha):\n",
    "    models = [linear_model.LinearRegression(optimizer=optim.SGD(lr=lr), loss=optim.losses.Quantile(alpha=alpha)) for lr in [0.05, 0.02, 0.01, 0.005, 0.002, 0.0001]]\n",
    "\n",
    "\n",
    "    model = compose.Pipeline(\n",
    "        ('features', compose.TransformerUnion(\n",
    "            ('date_progress', compose.FuncTransformer(get_date_progress))\n",
    "#             ('lags', compose.FuncTransformer(get_lag))\n",
    "        )))\n",
    "\n",
    "    model += (\n",
    "        get_hour | \n",
    "            feature_extraction.TargetAgg(\n",
    "                by=['h'], how=stats.Mean()\n",
    "\n",
    "\n",
    "    ))\n",
    "    # model += (\n",
    "    #     get_minute | \n",
    "    #         feature_extraction.TargetAgg(\n",
    "    #             by=['m'], how=stats.Mean()\n",
    "\n",
    "\n",
    "    # ))\n",
    "\n",
    "    model |=  preprocessing.StandardScaler()\n",
    "#     model |=  preprocessing.TargetStandardScaler(regressor=linear_model.LinearRegression(intercept_lr=0.15))\n",
    "    model |= linear_model.LinearRegression()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "models = {\n",
    "    'lower': make_model(alpha=0.05),\n",
    "    'center': make_model(alpha=0.5),\n",
    "    'upper': make_model(alpha=0.95)\n",
    "}\n",
    "\n",
    "dates = []\n",
    "y_trues = []\n",
    "y_preds = {\n",
    "    'lower': [],\n",
    "    'center': [],\n",
    "    'upper': []\n",
    "}\n",
    "\n",
    "for x, y in stream.iter_csv('basketball_dreb.csv', target = 'value', **params):\n",
    "    y_trues.append(y)\n",
    "    dates.append(x['time'])\n",
    "\n",
    "    for name, model in models.items():\n",
    "        y_preds[name].append(model.predict_one(x))\n",
    "        model.learn_one(x, y)\n",
    "\n",
    "    # Update the error metric\n",
    "    metric.update(y, y_preds['center'][-1])\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.grid(alpha=0.75)\n",
    "ax.plot(dates, y_trues, lw=3, color='#2ecc71', alpha=0.8, label='Truth')\n",
    "ax.plot(dates, y_preds['center'], lw=3, color='#e74c3c', alpha=0.8, label='Prediction')\n",
    "# ax.fill_between(dates, y_preds['lower'], y_preds['upper'], color='#e74c3c', alpha=0.3, label='Prediction interval')\n",
    "ax.legend()\n",
    "ax.set_title(metric);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
